{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(1867)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare MNIST Data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "#将数据切片，缓冲区1000，batch64\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((\n",
    "    tf.cast(x_train/255, tf.float32),\n",
    "    tf.cast(y_train, tf.int64)\n",
    ")).shuffle(1000).batch(64)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((\n",
    "    tf.cast(x_test/255, tf.float32),\n",
    "    tf.cast(y_test, tf.int64)\n",
    ")).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "model_orig = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 5, activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, 5, activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPool2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.75),\n",
    "    tf.keras.layers.Dense(10, use_bias=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracy = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "        epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "        for x, y in tqdm(dataset_train, total=round(len(x_train)/64)):\n",
    "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(x)\n",
    "                loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights), global_step)\n",
    "            epoch_loss_avg(loss)\n",
    "            epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "        training_losses.append(epoch_loss_avg.result())\n",
    "        training_accuracy.append(epoch_accuracy.result())\n",
    "\n",
    "    return training_losses, training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset):\n",
    "    epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "    epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "    for x, y in dataset:\n",
    "        x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        outputs = model(x)\n",
    "        loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "        epoch_loss_avg(loss)\n",
    "        epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "    return epoch_loss_avg.result().numpy(), epoch_accuracy.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:29<00:00, 31.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:28<00:00, 32.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:28<00:00, 33.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:28<00:00, 33.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:28<00:00, 32.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:28<00:00, 32.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:28<00:00, 32.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:28<00:00, 32.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:29<00:00, 31.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:28<00:00, 32.61it/s]\n"
     ]
    }
   ],
   "source": [
    "loss1, accuracies1 = train_model(model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_prune(dense_model, percentile):\n",
    "    conv_layer_cnt = 2\n",
    "    prev_kept_columns = None\n",
    "    pruned_model = tf.keras.models.Sequential()\n",
    "    num_layers = len(dense_model.trainable_weights)\n",
    "    filter_channel_norms = []\n",
    "    filter_norms = []\n",
    "    weights_thislayer_np = dense_model.trainable_weights[0].numpy()\n",
    "\n",
    "    #conv layer\n",
    "    for i_conv_layer in range(conv_layer_cnt):\n",
    "        weights_nextlayer_np = dense_model.trainable_weights[i_conv_layer*2+2].numpy()\n",
    "        for i_filter in range(weights_thislayer_np.shape[3]):\n",
    "            for d3 in range(weights_thislayer_np.shape[2]):\n",
    "                filter_channel_norms.append(np.linalg.norm(weights_thislayer_np[:, :, d3, i_filter], ord=2, axis=None))\n",
    "            filter_norms.append(np.linalg.norm(filter_channel_norms, ord=2))\n",
    "            filter_channel_norms.clear()\n",
    "        critical_value = np.percentile(filter_norms, percentile)\n",
    "        keep_mask = filter_norms >= critical_value\n",
    "        weights_thislayer_np = weights_thislayer_np[:, :, :, keep_mask]\n",
    "        if i_conv_layer < conv_layer_cnt-1 :\n",
    "            weights_nextlayer_np = weights_nextlayer_np[:, :, keep_mask, :]\n",
    "        else :\n",
    "            flatten_mask = np.zeros(shape=(len(keep_mask), len(weights_nextlayer_np)//len(keep_mask)))\n",
    "            flatten_mask[:] = np.array(keep_mask).reshape(-1, 1)\n",
    "            flatten_mask = flatten_mask.reshape(1, -1)\n",
    "            flatten_mask = flatten_mask[0]\n",
    "            weights_nextlayer_np = weights_nextlayer_np[np.argwhere(flatten_mask)[:, 0], :]\n",
    "        bias = dense_model.trainable_weights[i_conv_layer*2+1].numpy()[keep_mask]\n",
    "        if i_conv_layer == 0 :\n",
    "            new_layer = tf.keras.layers.Conv2D(weights_thislayer_np.shape[3], 5, activation=tf.nn.relu, input_shape=(28, 28, 1))\n",
    "        else :\n",
    "            new_layer = tf.keras.layers.Conv2D(weights_thislayer_np.shape[3], 5, activation=tf.nn.relu)\n",
    "        pruned_model.add(new_layer)\n",
    "        new_layer.set_weights([weights_thislayer_np, bias])\n",
    "        weights_thislayer_np = weights_nextlayer_np\n",
    "        filter_norms.clear()\n",
    "        \n",
    "        pruned_model.add(tf.keras.layers.MaxPool2D(2, 2))\n",
    "    \n",
    "    #flatten_layer\n",
    "    new_layer = tf.keras.layers.Flatten()\n",
    "    pruned_model.add(new_layer)\n",
    "    \n",
    "    new_layer = tf.keras.layers.Dense(1024, activation=tf.nn.relu)\n",
    "    pruned_model.add(new_layer)\n",
    "    new_layer.set_weights([weights_thislayer_np, dense_model.trainable_weights[5].numpy()])\n",
    "    #fc layer\n",
    "    pruned_model.add(tf.keras.layers.Dropout(0.75))\n",
    "    new_layer = tf.keras.layers.Dense(10, use_bias=False)\n",
    "    pruned_model.add(new_layer)\n",
    "    new_layer.set_weights(dense_model.layers[7].get_weights())\n",
    "    \n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = unit_prune(model_orig, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040772115297962735\n",
      "0.9875\n"
     ]
    }
   ],
   "source": [
    "loss_, accu_ = test(model_ft, dataset_test)\n",
    "print(loss_)\n",
    "print(accu_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(pruned_model):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracy = []\n",
    "\n",
    "    for epoch in range(4):\n",
    "        epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "        epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "        for x, y in tqdm(dataset_train, total=round(len(x_train)/64)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "                outputs = pruned_model(x)\n",
    "                loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "            grads = tape.gradient(loss, pruned_model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, pruned_model.trainable_weights), global_step)\n",
    "            epoch_loss_avg(loss)\n",
    "            epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "        training_losses.append(epoch_loss_avg.result())\n",
    "        training_accuracy.append(epoch_accuracy.result())\n",
    "\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 45.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 46.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 46.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:19<00:00, 46.98it/s]\n"
     ]
    }
   ],
   "source": [
    "model_ft = fine_tuning(p_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.trainable_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
