{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set()\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(1867)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare MNIST Data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "#将数据切片，缓冲区1000，batch64\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((\n",
    "    tf.cast(x_train/255, tf.float32),\n",
    "    tf.cast(y_train, tf.int64)\n",
    ")).shuffle(1000).batch(64)\n",
    "\n",
    "dateset_test = tf.data.Dataset.from_tensor_slices((\n",
    "    tf.cast(x_test/255, tf.float32),\n",
    "    tf.cast(y_test, tf.int64)\n",
    ")).batch(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "model_orig = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu, use_bias=False),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu, use_bias=False),\n",
    "    tf.keras.layers.Dense(500, activation=tf.nn.relu, use_bias=False),\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu, use_bias=False),\n",
    "    tf.keras.layers.Dense(10, use_bias=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracy = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "        epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "        for x, y in tqdm(dataset_train, total=round(len(x_train)/64)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(x)\n",
    "                loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights), global_step)\n",
    "            epoch_loss_avg(loss)\n",
    "            epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "        training_losses.append(epoch_loss_avg.result())\n",
    "        training_accuracy.append(epoch_accuracy.result())\n",
    "\n",
    "    return training_losses, training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_train(pruned_model):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracy = []\n",
    "\n",
    "    for epoch in range(1):\n",
    "        epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "        epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "        for x, y in tqdm(dataset_train, total=round(len(x_train)/64)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = pruned_model(x)\n",
    "                loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "            grads = tape.gradient(loss, pruned_model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, pruned_model.trainable_weights), global_step)\n",
    "            epoch_loss_avg(loss)\n",
    "            epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "        training_losses.append(epoch_loss_avg.result())\n",
    "        training_accuracy.append(epoch_accuracy.result())\n",
    "\n",
    "    return training_losses, training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=179433, shape=(), dtype=float64, numpy=0.9418833333333333>,\n",
       " <tf.Tensor: id=358650, shape=(), dtype=float64, numpy=0.97235>,\n",
       " <tf.Tensor: id=537867, shape=(), dtype=float64, numpy=0.9797166666666667>,\n",
       " <tf.Tensor: id=717084, shape=(), dtype=float64, numpy=0.985>,\n",
       " <tf.Tensor: id=896301, shape=(), dtype=float64, numpy=0.9876333333333334>,\n",
       " <tf.Tensor: id=1075518, shape=(), dtype=float64, numpy=0.98935>,\n",
       " <tf.Tensor: id=1254735, shape=(), dtype=float64, numpy=0.9901333333333333>,\n",
       " <tf.Tensor: id=1433952, shape=(), dtype=float64, numpy=0.9914833333333334>,\n",
       " <tf.Tensor: id=1613169, shape=(), dtype=float64, numpy=0.9926>,\n",
       " <tf.Tensor: id=1792386, shape=(), dtype=float64, numpy=0.9929833333333333>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(model, dataset):\n",
    "    epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "    epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "    for x, y in dataset:\n",
    "        outputs = model(x)\n",
    "        loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "        epoch_loss_avg(loss)\n",
    "        epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "    return epoch_loss_avg.result().numpy(), epoch_accuracy.result().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10114042613807853, 0.9795)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unit_prune(dense_model, percentile):\n",
    "    prev_kept_columns = None\n",
    "    pruned_model = tf.keras.models.Sequential()\n",
    "    pruned_model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "    num_layers = len(dense_model.trainable_weights)\n",
    "\n",
    "    for i_layer, weights in enumerate(dense_model.trainable_weights):\n",
    "        weights_np = weights.numpy()\n",
    "\n",
    "        # Remove pruned columns\n",
    "        if i_layer < num_layers - 1:  # Do not prune last layer\n",
    "            column_norms = np.linalg.norm(weights_np, ord=2, axis=0)\n",
    "            critical_value = np.percentile(column_norms, percentile)\n",
    "            keep_mask = column_norms >= critical_value\n",
    "            weights_np = weights_np[:, keep_mask]\n",
    "\n",
    "        # Remove rows corresponding to previous layer's pruned columns\n",
    "        if prev_kept_columns is not None:\n",
    "            weights_np = weights_np[prev_kept_columns, :]\n",
    "\n",
    "        # Record which columns were kept\n",
    "        if i_layer < num_layers - 1:  # No pruned columns in last layer\n",
    "            prev_kept_columns = np.argwhere(keep_mask).reshape(-1)\n",
    "\n",
    "        # Add new layer to sparse model\n",
    "        new_layer = tf.keras.layers.Dense(weights_np.shape[1], activation=tf.nn.relu, use_bias=False)\n",
    "        pruned_model.add(new_layer)\n",
    "        new_layer.set_weights([weights_np])\n",
    "\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:24<00:00, 37.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:22<00:00, 42.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:21<00:00, 44.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:21<00:00, 44.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:21<00:00, 44.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 45.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 45.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:21<00:00, 44.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 45.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 45.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=4405096, shape=(), dtype=float64, numpy=0.19671311230956715>, <tf.Tensor: id=4584313, shape=(), dtype=float64, numpy=0.09328903942903839>, <tf.Tensor: id=4763530, shape=(), dtype=float64, numpy=0.06762130413989303>, <tf.Tensor: id=4942747, shape=(), dtype=float64, numpy=0.05142297952891147>, <tf.Tensor: id=5121964, shape=(), dtype=float64, numpy=0.039816764571163406>, <tf.Tensor: id=5301181, shape=(), dtype=float64, numpy=0.037109097587299036>, <tf.Tensor: id=5480398, shape=(), dtype=float64, numpy=0.03431974684613929>, <tf.Tensor: id=5659615, shape=(), dtype=float64, numpy=0.029289227981580998>, <tf.Tensor: id=5838832, shape=(), dtype=float64, numpy=0.025262298133229184>, <tf.Tensor: id=6018049, shape=(), dtype=float64, numpy=0.024636659453119374>]\n",
      "[<tf.Tensor: id=4405102, shape=(), dtype=float64, numpy=0.9418833333333333>, <tf.Tensor: id=4584319, shape=(), dtype=float64, numpy=0.97235>, <tf.Tensor: id=4763536, shape=(), dtype=float64, numpy=0.9797166666666667>, <tf.Tensor: id=4942753, shape=(), dtype=float64, numpy=0.985>, <tf.Tensor: id=5121970, shape=(), dtype=float64, numpy=0.9876333333333334>, <tf.Tensor: id=5301187, shape=(), dtype=float64, numpy=0.98935>, <tf.Tensor: id=5480404, shape=(), dtype=float64, numpy=0.9901333333333333>, <tf.Tensor: id=5659621, shape=(), dtype=float64, numpy=0.9914833333333334>, <tf.Tensor: id=5838838, shape=(), dtype=float64, numpy=0.9926>, <tf.Tensor: id=6018055, shape=(), dtype=float64, numpy=0.9929833333333333>]\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies = train_model(model_orig)\n",
    "print(losses)\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_ = unit_prune(model_orig, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_pruned, accuracies_pruned = test(pruned_model_, dateset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2603668680616247\n",
      "0.5594\n"
     ]
    }
   ],
   "source": [
    "print(losses_pruned)\n",
    "print(accuracies_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:14<00:00, 63.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=6994803, shape=(), dtype=float64, numpy=0.01900134881276799>]\n",
      "[<tf.Tensor: id=6994809, shape=(), dtype=float64, numpy=0.9944666666666667>]\n"
     ]
    }
   ],
   "source": [
    "losses_retrained, accuracies_retrained = re_train(pruned_model_)\n",
    "print(losses_retrained)\n",
    "print(accuracies_retrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09755854798758537\n",
      "0.9777\n"
     ]
    }
   ],
   "source": [
    "losses1, accuracies1 = test(pruned_model_, dateset_test)\n",
    "print(losses1)\n",
    "print(accuracies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
