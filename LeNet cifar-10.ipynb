{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#tf.device('/gpu:1')\n",
    "\n",
    "sns.set()\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(1867)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 1105s 6us/step\n"
     ]
    }
   ],
   "source": [
    "#Prepare MNIST Data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#将数据切片，缓冲区1000，batch64\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((\n",
    "    tf.cast(x_train/255, tf.float32),\n",
    "    tf.cast(y_train, tf.int64)\n",
    ")).shuffle(1000).batch(64)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((\n",
    "    tf.cast(x_test/255, tf.float32),\n",
    "    tf.cast(y_test, tf.int64)\n",
    ")).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "model_orig = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 5, activation=tf.nn.relu, input_shape=(32, 32, 3), kernel_regularizer=tf.keras.regularizers.l1(0.001)),\n",
    "    tf.keras.layers.MaxPool2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, 5, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l1(0.001)),\n",
    "    tf.keras.layers.MaxPool2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.75),\n",
    "    tf.keras.layers.Dense(10, use_bias=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracy = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "        epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "        for x, y in tqdm(dataset_train, total=round(len(x_train)/64)):\n",
    "            x = tf.reshape(x, [-1, 32, 32, 3])\n",
    "            y = y[:, 0]\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(x)\n",
    "                loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights), global_step)\n",
    "            epoch_loss_avg(loss)\n",
    "            epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "        training_losses.append(epoch_loss_avg.result())\n",
    "        training_accuracy.append(epoch_accuracy.result())\n",
    "\n",
    "    return training_losses, training_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset):\n",
    "    epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "    epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "    for x, y in dataset:\n",
    "        x = tf.reshape(x, [-1, 32, 32, 3])\n",
    "        y = y[:, 0]\n",
    "        outputs = model(x)\n",
    "        loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "        epoch_loss_avg(loss)\n",
    "        epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "    return epoch_loss_avg.result().numpy(), epoch_accuracy.result().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "def unit_prune(dense_model, percentile):\n",
    "    conv_layer_cnt = 2\n",
    "    pruned_model = tf.keras.models.Sequential()\n",
    "    filter_channel_norms = []\n",
    "    filter_norms = []\n",
    "    weights_thislayer_np = dense_model.trainable_weights[0].numpy()\n",
    "\n",
    "    # conv layer\n",
    "    for i_conv_layer in range(conv_layer_cnt):\n",
    "        weights_nextlayer_np = dense_model.trainable_weights[i_conv_layer * 2 + 2].numpy()\n",
    "        for i_filter in range(weights_thislayer_np.shape[3]):\n",
    "            filter_norms.append(np.mean(np.fabs(weights_thislayer_np[:, :, :, i_filter])))\n",
    "        critical_value = np.percentile(filter_norms, percentile)\n",
    "        keep_mask = filter_norms >= critical_value\n",
    "        weights_thislayer_np = weights_thislayer_np[:, :, :, keep_mask]\n",
    "        if i_conv_layer < conv_layer_cnt - 1:\n",
    "            weights_nextlayer_np = weights_nextlayer_np[:, :, keep_mask, :]\n",
    "        else:\n",
    "            flatten_mask = np.zeros(shape=(len(keep_mask), len(weights_nextlayer_np) // len(keep_mask)))\n",
    "            flatten_mask[:] = np.array(keep_mask).reshape(-1, 1)\n",
    "            flatten_mask = flatten_mask.reshape(1, -1)\n",
    "            flatten_mask = flatten_mask[0]\n",
    "            weights_nextlayer_np = weights_nextlayer_np[np.argwhere(flatten_mask)[:, 0], :]\n",
    "        bias = dense_model.trainable_weights[i_conv_layer * 2 + 1].numpy()[keep_mask]\n",
    "        if i_conv_layer == 0:\n",
    "            new_layer = tf.keras.layers.Conv2D(weights_thislayer_np.shape[3], 5, activation=tf.nn.relu,\n",
    "                                               input_shape=(32, 32, 1))\n",
    "        else:\n",
    "            new_layer = tf.keras.layers.Conv2D(weights_thislayer_np.shape[3], 5, activation=tf.nn.relu)\n",
    "        pruned_model.add(new_layer)\n",
    "        new_layer.set_weights([weights_thislayer_np, bias])\n",
    "        weights_thislayer_np = weights_nextlayer_np\n",
    "        filter_norms.clear()\n",
    "        pruned_model.add(tf.keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    # flatten_layer\n",
    "    new_layer = tf.keras.layers.Flatten()\n",
    "    pruned_model.add(new_layer)\n",
    "\n",
    "    new_layer = tf.keras.layers.Dense(1024, activation=tf.nn.relu)\n",
    "    pruned_model.add(new_layer)\n",
    "    new_layer.set_weights([weights_thislayer_np, dense_model.trainable_weights[5].numpy()])\n",
    "    # fc layer\n",
    "    pruned_model.add(tf.keras.layers.Dropout(0.75))\n",
    "    new_layer = tf.keras.layers.Dense(10, use_bias=False)\n",
    "    pruned_model.add(new_layer)\n",
    "    new_layer.set_weights(dense_model.layers[7].get_weights())\n",
    "\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(pruned_model, epoch_num):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracy = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_loss_avg = tf.contrib.eager.metrics.Mean()\n",
    "        epoch_accuracy = tf.contrib.eager.metrics.Accuracy()\n",
    "        for x, y in tqdm(dataset_train, total=round(len(x_train)/64)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                x = tf.reshape(x, [-1, 32, 32, 3])\n",
    "                y = y[:, 0]\n",
    "                outputs = pruned_model(x)\n",
    "                loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), outputs)\n",
    "            grads = tape.gradient(loss, pruned_model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, pruned_model.trainable_weights), global_step)\n",
    "            epoch_loss_avg(loss)\n",
    "            epoch_accuracy(tf.argmax(outputs, axis=1, output_type=tf.int64), y)\n",
    "        training_losses.append(epoch_loss_avg.result())\n",
    "        training_accuracy.append(epoch_accuracy.result())\n",
    "\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [00:07, 103.73it/s]                         \n",
      "782it [00:08, 91.73it/s]                          \n",
      "782it [00:07, 103.24it/s]                         \n",
      "782it [00:07, 103.97it/s]                         \n",
      "782it [00:07, 104.23it/s]                         \n",
      "782it [00:07, 100.09it/s]                         \n",
      "782it [00:07, 104.31it/s]                         \n",
      "782it [00:07, 103.85it/s]                         \n",
      "782it [00:08, 92.38it/s]                          \n",
      "782it [00:07, 103.27it/s]                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: id=1713754, shape=(), dtype=float64, numpy=0.1430478488652469>,\n",
       "  <tf.Tensor: id=1867861, shape=(), dtype=float64, numpy=0.11510063390976386>,\n",
       "  <tf.Tensor: id=2021968, shape=(), dtype=float64, numpy=0.11258356908009962>,\n",
       "  <tf.Tensor: id=2176075, shape=(), dtype=float64, numpy=0.1015678774129273>,\n",
       "  <tf.Tensor: id=2330182, shape=(), dtype=float64, numpy=0.09721637058995969>,\n",
       "  <tf.Tensor: id=2484289, shape=(), dtype=float64, numpy=0.08750068436405334>,\n",
       "  <tf.Tensor: id=2638396, shape=(), dtype=float64, numpy=0.07658848277253845>,\n",
       "  <tf.Tensor: id=2792503, shape=(), dtype=float64, numpy=0.0721656753707801>,\n",
       "  <tf.Tensor: id=2946610, shape=(), dtype=float64, numpy=0.07511396806203591>,\n",
       "  <tf.Tensor: id=3100717, shape=(), dtype=float64, numpy=0.06678990963900037>],\n",
       " [<tf.Tensor: id=1713759, shape=(), dtype=float64, numpy=0.95076>,\n",
       "  <tf.Tensor: id=1867866, shape=(), dtype=float64, numpy=0.96042>,\n",
       "  <tf.Tensor: id=2021973, shape=(), dtype=float64, numpy=0.96294>,\n",
       "  <tf.Tensor: id=2176080, shape=(), dtype=float64, numpy=0.9657>,\n",
       "  <tf.Tensor: id=2330187, shape=(), dtype=float64, numpy=0.9675>,\n",
       "  <tf.Tensor: id=2484294, shape=(), dtype=float64, numpy=0.9715>,\n",
       "  <tf.Tensor: id=2638401, shape=(), dtype=float64, numpy=0.97462>,\n",
       "  <tf.Tensor: id=2792508, shape=(), dtype=float64, numpy=0.97658>,\n",
       "  <tf.Tensor: id=2946615, shape=(), dtype=float64, numpy=0.97664>,\n",
       "  <tf.Tensor: id=3100722, shape=(), dtype=float64, numpy=0.97898>])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pruned = unit_prune(model_orig, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:07<00:00, 119.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.048614128166201705, 0.9856)\n"
     ]
    }
   ],
   "source": [
    "model_ft = fine_tuning(model_ft, 1)\n",
    "print(test(model_ft, dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:07<00:00, 118.55it/s]\n",
      "  0%|          | 2/938 [00:00<00:49, 18.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.02926730891900012, 0.9887)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:07<00:00, 120.16it/s]\n",
      "  0%|          | 2/938 [00:00<00:50, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.03253549118105016, 0.9882)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:07<00:00, 119.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.04158904143478603, 0.9847)\n"
     ]
    }
   ],
   "source": [
    "model_pruned_30 = unit_prune(model_orig, 30)\n",
    "model_ft_30 = fine_tuning(model_pruned_30, 1)\n",
    "print(test(model_ft_30, dataset_test))\n",
    "\n",
    "model_pruned_30 = unit_prune(model_ft_30, 30/0.7)\n",
    "model_ft_30 = fine_tuning(model_pruned_30, 1)\n",
    "print(test(model_ft_30, dataset_test))\n",
    "\n",
    "model_pruned_30 = unit_prune(model_ft_30, 30/0.7/0.7)\n",
    "model_ft_30 = fine_tuning(model_pruned_30, 1)\n",
    "print(test(model_ft_30, dataset_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(5), Dimension(1), Dimension(5)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft_30.trainable_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7692928458475006\n",
      "0.6656\n"
     ]
    }
   ],
   "source": [
    "loss_orig, accu_orig = test(model_orig, dataset_test)\n",
    "print(loss_orig)\n",
    "print(accu_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
